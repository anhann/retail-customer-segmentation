# -*- coding: utf-8 -*-
"""Evaluation code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HMyb9ht_4CpC63NrNqGoaw4XV22gUCJw
"""

#Import packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### Feature Generation"""

#Loading data
cate=pd.read_csv('%cate_clean_uncross.csv')
others=pd.read_csv('all_except_cate_final.csv')

#Cleaning data
cate=cate.pivot(index="customer_number", columns="category", values="cate_contribution")
cate= cate.fillna(0)
others= others.fillna(0)
df=others.merge(cate, left_on='customer_number', right_on='customer_number')
data=df.iloc[:,1:]

#Checking data
data.describe()

"""### Feature Selection"""

#Plot heat map for all features
plt.figure(figsize=(50,30))
g=sns.heatmap(data.corr(),annot=True,cmap="Blues",annot_kws={"size": 16})

#Plot heat map for Shopping mission features
unit=data[['unit_cost','tobacco','cashpoint','drinks','dairy','confectionary','bakery','grocery_food','fruit_veg','grocery_health_pets']]
plt.figure(figsize=(12,8))
g=sns.heatmap(unit.corr(),annot=True,cmap="Blues",annot_kws={"size": 12})

#Plot heat map for Shopping mission features - no number
plt.figure(figsize=(12,8))
g=sns.heatmap(unit.corr(),annot=True,cmap="Blues",annot_kws={"size": 0})

#Merging mega-categories 
high_value=data['cashpoint']+ data['drinks'] + data ['lottery'] + data ['tobacco']
daily_ess=data['bakery'] + data['confectionary']+data['dairy'] + data['frozen'] + data['fruit_veg']+data['grocery_food']+data['grocery_health_pets'] +data['meat']+data['prepared_meals']
non_ess=data['discount_bakery']+data['newspapers_magazines']+data['practical_items']+data['seasonal_gifting']+data['soft_drinks']+data['deli']+data['world_foods']
cate_new=pd.concat([high_value,daily_ess,non_ess],axis=1)
cate_new.rename(columns={cate_new.columns[0]:'high_value'},inplace=True)
cate_new.rename(columns={cate_new.columns[1]:'daily_essential'},inplace=True)
cate_new.rename(columns={cate_new.columns[2]:'non_essential'},inplace=True)

#Checking correlation of mega-cates 
#High value negatively correlated with daily essential
plt.figure(figsize=(5,3))
g=sns.heatmap(cate_new.corr(),annot=True,cmap="Blues",annot_kws={"size": 10})

#Dropping highly correlated features (abs >0.7) including daily_essential
unbias_data=data.drop(columns=['total_quantity','weekly_spend','weekly_visit','basket_value','num_pro'])
new_data=pd.concat([unbias_data.iloc[:,:9],cate_new.loc[:,cate_new.columns!='daily_essential']],axis=1)
new_data.corr()

"""### Feature Transformation"""

#Checking data distribution
new_data.hist(figsize=(12,10), color='grey')

#Tranforming using Logarithm and Standardization
from sklearn.preprocessing import StandardScaler
scl=StandardScaler()
import numpy as np
new_log=np.log(new_data+1)
new_norm=pd.DataFrame(scl.fit_transform(new_log))
for i in range(len(new_norm.columns)):
  new_norm.rename(columns={new_norm.columns[i]:new_data.columns[i]},inplace=True)
new_norm.hist(figsize=(15,10))

#Tranforming using Yeo-Johnson and Standardization
#Result better than Log+Standard
from sklearn.preprocessing import power_transform
new_box=pd.DataFrame(power_transform(new_data, method='yeo-johnson',standardize=True))
for i in range(len(new_box.columns)):
  new_box.rename(columns={new_box.columns[i]:new_data.columns[i]},inplace=True)
new_box.hist(figsize=(12,10))

"""### Feature Extraction """

#PCA Visualization function - Professor provided function
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

def pca_results(good_data, pca):
  '''
  Create a DataFrame of the PCA results
  Includes dimension feature weights and explained variance
  Visualizes the PCA results
  '''

  # Dimension indexing
  dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]

  # PCA components
  components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())
  components.index = dimensions

  # PCA explained variance
  ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)
  variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])
  variance_ratios.index = dimensions

  # Create a bar plot visualization
  fig, ax = plt.subplots(figsize = (14,8))

  # Plot the feature weights as a function of the components
  components.plot(ax = ax, kind = 'bar');
  ax.set_ylabel("Feature Weights")
  ax.set_xticklabels(dimensions, rotation=0)


  # Display the explained variance ratios
  for i, ev in enumerate(pca.explained_variance_ratio_):
    ax.text(i-0.40, ax.get_ylim()[1] + 0.05, "Explained Variance\n          %.4f"%(ev))

  # Return a concatenated DataFrame
  return pd.concat([variance_ratios, components], axis = 1)


def cluster_results(reduced_data, preds, centers):
  '''
  Visualizes the PCA-reduced cluster data in two dimensions
  Adds cues for cluster centers and student-selected sample data
  '''

  predictions = pd.DataFrame(preds, columns = ['Cluster'])
  plot_data = pd.concat([predictions, reduced_data], axis = 1)

  # Generate the cluster plot
  fig, ax = plt.subplots(figsize = (14,8))

  # Color map
  cmap = cm.get_cmap('gist_rainbow')

  # Color the points based on assigned cluster
  for i, cluster in plot_data.groupby('Cluster'):   
      cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \
                   color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);

  # Plot centers with indicators
  for i, c in enumerate(centers):
      ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \
                 alpha = 1, linewidth = 2, marker = 'o', s=200);
      ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);

  # Set plot title
  ax.set_title("Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\nTransformed Sample Data Marked by Black Cross");


def channel_results(reduced_data):
  '''
  Visualizes the PCA-reduced cluster data in two dimensions using the full dataset
  Data is labeled by "Channel" and cues added for student-selected sample data
  '''

  # Check that the dataset is loadable
  try:
      full_data = pd.read_csv("customers.csv")
  except:
      print("Dataset could not be loaded. Is the file missing?")
      return False

  # Create the Channel DataFrame
  channel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])
  labeled = pd.concat([reduced_data, channel], axis = 1)
  
  # Generate the cluster plot
  fig, ax = plt.subplots(figsize = (14,8))

  # Color map
  cmap = cm.get_cmap('gist_rainbow')

  # Color the points based on assigned Channel
  labels = ['Hotel/Restaurant/Cafe', 'Retailer']
  grouped = labeled.groupby('Channel')
  for i, channel in grouped:   
      channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \
                   color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);

  # Set plot title
  ax.set_title("PCA-Reduced Data Labeled by 'Channel'\nTransformed Sample Data Circled");

#Fitting PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=10, random_state=42)
pca.fit(new_box)
pca_results = pca_results(new_box,pca)
display(pca_results['Explained Variance'].cumsum())

#Rule of thumb - selecting 6 components
plt.rcParams["figure.figsize"] = (15,9)
elbow=pca_results['Explained Variance'].cumsum()
plt.plot(elbow,'.-')

pca = PCA(n_components=6, random_state=42)
reduced_data=pca.fit_transform(new_box)
scatter = pd.plotting.scatter_matrix(pd.DataFrame(reduced_data), figsize = (15,10))

"""### Model selection"""

#Iterating k-Means with cluster number from 3 to 10 - calculate Inertia/WSS & Silhouette score
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
km_score= []
km_silhouette = []
for i in range(3,11):
    km = KMeans(n_clusters=i, random_state=42).fit(reduced_data)
    preds = km.predict(reduced_data)
    
    print("Score for number of cluster(s) {}: {}".format(i,km.score(reduced_data)))
    km_score.append(-km.score(reduced_data))
    
    silhouette = silhouette_score(reduced_data,preds)
    km_silhouette.append(silhouette)
    print("Silhouette score for number of cluster(s) {}: {}".format(i,silhouette))

#Plotting WSS/ Inertia
plt.figure(figsize=(7,4))
plt.scatter(x=[i for i in range(3,11)],y=km_score,s=70,color='steelblue')
plt.grid(True)
plt.xlabel("Number of clusters",fontsize=10)
plt.ylabel("WSS",fontsize=10)
plt.xticks([i for i in range(3,11)],fontsize=10)
plt.yticks(fontsize=10)
plt.show()

#Plotting Silhouette
plt.figure(figsize=(7,4))
plt.scatter(x=[i for i in range(3,11)],y=km_silhouette,s=70, color='steelblue')
plt.grid(True)
plt.xlabel("Number of clusters",fontsize=10)
plt.xticks([i for i in range(3,11)],fontsize=10)
plt.ylabel("Silhouette score",fontsize=10)
plt.show()

#Sihouette Visualization function 
#Credit with revision to https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
def SihouetteVisual(range_n_clusters,X):
    from sklearn.metrics import silhouette_samples, silhouette_score
    import matplotlib.cm as cm
    for n_clusters in range_n_clusters:
        # Create a subplot with 1 row and 2 columns
        fig, (ax1) = plt.subplots(1, 1)
        fig.set_size_inches(18, 7)


        # The (n_clusters+1)*10 is for inserting blank space between silhouette
        # plots of individual clusters, to demarcate them clearly.
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        # Initialize the clusterer with n_clusters value and a random generator
        # seed of 10 for reproducibility.
        clusterer = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = clusterer.fit_predict(X)

        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg = silhouette_score(X, cluster_labels)
        print("For n_clusters =", n_clusters,
              "The average silhouette_score is :", silhouette_avg)

        # Compute the silhouette scores for each sample
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            # Aggregate the silhouette scores for samples belonging to
            # cluster i, and sort them
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.nipy_spectral(float(i) / n_clusters)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_cluster_silhouette_values, alpha=0.7)

            # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples

        ax1.set_title("The silhouette plot for the various clusters.")
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color="grey", linestyle="-.")

        ax1.set_yticks([])  # Clear the yaxis labels / ticks
        ax1.set_xticks([-0.1, 0, 0.2, 0.4])
        plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                      "with n_clusters = %d" % n_clusters),
                     fontsize=14, fontweight='bold')

    plt.show()
range_n_clusters = range(3,11)
SihouetteVisual(range_n_clusters,reduced_data)

"""### Clustering Results """

#Generating segments
clusterer = KMeans(n_clusters=5, random_state=42).fit(reduced_data)
preds = clusterer.predict(reduced_data)
assignments = pd.DataFrame(preds, columns = ['Cluster'])
final_assigments = pd.concat([assignments, df], axis = 1)
for c, d in final_assigments.groupby('Cluster'):  
    print("SEGMENT", c)
    display(d.describe())

#Polar chart for shopping behaviours by cluster
!pip install plotly
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
key=pd.DataFrame(final_assigments[['Cluster','total_spend','total_visit','basket_product', 'basket_value','unit_cost','last_purchase','num_cate','spend_change','weekend_percent']])

key_standard=pd.DataFrame(scaler.fit_transform(key.iloc[:,1:]))
key_standard=pd.concat([pd.DataFrame(key['Cluster']),key_standard],axis=1)
key_standard
for i in range(len(key_standard.columns)):
  key_standard.rename(columns={key_standard.columns[i]:key.columns[i]},inplace=True)
key_standard.describe()
import plotly.express as px
polar=key_standard.groupby("Cluster").mean().reset_index()
polar=pd.melt(polar,id_vars=["Cluster"])
fig = px.line_polar(polar, r="value", theta="variable", color="Cluster", line_close=True,
                    color_discrete_map={3: "royalblue ",
                                        4: "mediumseagreen",
                                        0: "orangered",
                                        2: "orange",
                                        1: "mediumorchid"}
                                  )
fig.show()

#Polar chart for shopping mission by cluster
cate=pd.DataFrame(final_assigments.loc[:,'bakery':])
cate=pd.concat([pd.DataFrame(final_assigments['Cluster']),cate],axis=1)
cate_standard=pd.DataFrame(scaler.fit_transform(cate.iloc[:,1:]))
cate_standard=pd.concat([pd.DataFrame(cate['Cluster']),cate_standard],axis=1)
for i in range(len(cate_standard.columns)):
  cate_standard.rename(columns={cate_standard.columns[i]:cate.columns[i]},inplace=True)
cate_standard.describe()
polar=cate_standard.groupby("Cluster").mean().reset_index()
polar=pd.melt(polar,id_vars=["Cluster"])
fig = px.line_polar(polar, r="value", theta="variable", color="Cluster", line_close=True,
                    color_discrete_map={3: "royalblue ",
                                        4: "mediumseagreen",
                                        0: "orangered",
                                        2: "orange",
                                        1: "mediumorchid"})
fig.show()

#Changing cluster names
final_assigments.loc[final_assigments['Cluster']==0,'Cluster']='potential_lapsers'
final_assigments.loc[final_assigments['Cluster']==1,'Cluster']='economical_neighbours'
final_assigments.loc[final_assigments['Cluster']==2,'Cluster']='onestop_shoppers'
final_assigments.loc[final_assigments['Cluster']==3,'Cluster']='super_customers'
final_assigments.loc[final_assigments['Cluster']==4,'Cluster']='bustling_homemakers'

#Saving file
pd.DataFrame(final_assigments[['Cluster','customer_number']]).to_csv('cluster_id.csv',index=False)

